# -*- coding: utf-8 -*-
"""langchain_lcel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OL2Jb2gi8Dd15DFq1h37mZogNBAkf1GA
"""

!pip install langchain-openai==0.2.0 tavily-python==0.5.0 langchain

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = userdata.get("LANGCHAIN_API_KEY")
os.environ["LANGCHAIN_PROJECT"] = "agent-book"

os.environ["TAVILY_API_KEY"] = userdata.get("TAVILY_API_KEY")

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ユーザが入力した料理のレシピを考えてください。"),
        ("human", "{dish}"),
    ]
)

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

# prompt_value = prompt.invoke({"dish": "カレー"})
# ai_message = model.invoke(prompt_value)
# output = output_parser.invoke(ai_message)

chain = prompt|model|output_parser
output = chain.invoke({"dish": "カレー"})

print(output)

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

cot_prompt = ChatPromptTemplate.from_messages(
  [
      ("system", "ユーザの質問にステップバイステップで回答してください。"),
      ("human", "{question}"),
  ]
)

cot_chain = cot_prompt|model|output_parser

summarize_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ステップバイステップで考えた回答から結論だけ抽出してください。"),
        ("human", "{text}"),
    ]
)

summarize_chain = summarize_prompt|model|output_parser

chain = cot_chain|summarize_chain

chain.invoke({"question": "10+2*3"})

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "{input}"),
    ]
)

def upper(text: str) -> str:
  return text.upper()

chain = prompt|model|output_parser|RunnableLambda(upper)

output = chain.invoke({"input": "Hello!"})
print(output)

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import chain

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "{input}"),
    ]
)

@chain
def upper(text: str) -> str:
  return text.upper()

chain = prompt|model|output_parser|upper

output = chain.invoke({"input": "Hello!"})
print(output)

import pprint
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableParallel

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

optimistic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたは楽観主義者です。ユーザの入力に対して楽観的な意見をください。"),
        ("human", "{topic}"),
    ]
)

optimistic_chain = optimistic_prompt|model|output_parser

pessimistic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたは悲観主義者です。ユーザの入力に対して悲観的な意見をください。"),
        ("human", "{topic}"),
    ]
)

pessimistic_chain = pessimistic_prompt|model|output_parser

parallel_chain = RunnableParallel(
    {
        "optimistic_opinion": optimistic_chain,
        "pessimistic_opinion": pessimistic_chain,
    }
)

output = parallel_chain.invoke({"topic": "生成AIの進化について"})
pprint.pprint(output)

import pprint
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableParallel

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

optimistic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたは楽観主義者です。ユーザの入力に対して楽観的な意見をください。"),
        ("human", "{topic}"),
    ]
)

optimistic_chain = optimistic_prompt|model|output_parser

pessimistic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたは悲観主義者です。ユーザの入力に対して悲観的な意見をください。"),
        ("human", "{topic}"),
    ]
)

pessimistic_chain = pessimistic_prompt|model|output_parser

parallel_chain = RunnableParallel(
    {
        "optimistic_opinion": optimistic_chain,
        "pessimistic_opinion": pessimistic_chain,
    }
)

synthesize_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたは客観的なAIです。２つの意見をまとめてください。"),
        ("human", "楽観的意見：{optimistic_opinion}\n悲観的意見：{pessimistic_opinion}"),
    ]
)

synthesize_chain = synthesize_prompt|model|output_parser

chain = parallel_chain|synthesize_chain
chain.invoke({"topic": "生成AIの進化について"})

import pprint
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

optimistic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたは楽観主義者です。ユーザの入力に対して楽観的な意見をください。"),
        ("human", "{topic}について"),
    ]
)

optimistic_chain = optimistic_prompt|model|output_parser

pessimistic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたは悲観主義者です。ユーザの入力に対して悲観的な意見をください。"),
        ("human", "{topic}について"),
    ]
)

pessimistic_chain = pessimistic_prompt|model|output_parser

synthesize_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたは客観的なAIです。{topic}について２つの意見をまとめてください。"),
        ("human", "楽観的意見：{optimistic_opinion}\n悲観的意見：{pessimistic_opinion}"),
    ]
)

synthesize_chain = synthesize_prompt|model|output_parser

chain = {
  "optimistic_opinion": optimistic_chain,
  "pessimistic_opinion": pessimistic_chain,
  "topic": itemgetter("topic"),
}|synthesize_chain

chain.invoke({"topic": "生成AIの進化"})

import pprint
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.retrievers.tavily_search_api import TavilySearchAPIRetriever
from langchain_core.runnables import RunnablePassthrough

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

prompt = ChatPromptTemplate.from_template('''\
以下の文脈だけを踏まえて質問に回答してください。

文脈："""
{context}
"""

質問：{question}
''')

retriever = TavilySearchAPIRetriever(k=3)

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    |prompt
    |model
    |output_parser
)

output = chain.invoke("東京の今日の天気は？")
print(output)

from uuid import uuid4
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import SQLChatMessageHistory

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)

chain = prompt|model|output_parser

def respond(session_id: str, human_message: str) -> str:
  chat_message_history = SQLChatMessageHistory(
      session_id=session_id, connection="sqlite:///sqlite.db"
  )
  messages = chat_message_history.get_messages()

  ai_message = chain.invoke(
      {
          "chat_history": messages,
          "input": human_message,
      }
  )

  chat_message_history.add_user_message(human_message)
  chat_message_history.add_ai_message(ai_message)

  return ai_message

session_id = uuid4().hex

output1 = respond(
    session_id, human_message="こんにちは！私はジョンと言います！",
)
print(output1)

output2 = respond(
    session_id, human_message="私の名前がわかりますか？",
)
print(output2)