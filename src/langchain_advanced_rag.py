# -*- coding: utf-8 -*-
"""langchain_advanced_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BJr9yL58symP7O9Bw2AT_yDd5rlowAEQ
"""

!pip install langchain-openai==0.2.0 tavily-python==0.5.0 langchain-core==0.3.0 langchain-community==0.3.0 GitPython==3.1.43 langchain-chroma==0.1.4 langchain-cohere==0.3.0 rank-bm25==0.2.2

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = userdata.get("LANGCHAIN_API_KEY")
os.environ["LANGCHAIN_PROJECT"] = "agent-book"

os.environ["TAVILY_API_KEY"] = userdata.get("TAVILY_API_KEY")
os.environ["COHERE_API_KEY"] = userdata.get("COHERE_API_KEY")

from langchain_community.document_loaders import GitLoader
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

def file_filter(file_path: str) -> bool:
  return file_path.endswith(".mdx")

loader = GitLoader(
    clone_url="https://github.com/langchain-ai/langchain",
    repo_path="./langchain",
    branch="master",
    file_filter=file_filter,
)

documents = loader.load()

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
db = Chroma.from_documents(documents, embeddings)

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

prompt = ChatPromptTemplate.from_template('''\
以下の文脈だけを踏まえて質問に回答してください。

文脈："""
{context}
"""

質問：{question}
''')

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

retriever = db.as_retriever()

chain = {
    "question": RunnablePassthrough(),
    "context": retriever,
}|prompt|model|StrOutputParser()

chain.invoke("LangChainの概要を教えて")

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

hypothetical_prompt = ChatPromptTemplate.from_template("""\
次の質問に回答する一文を書いてください。

質問：{question}
""")

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

hypothetical_chain = hypothetical_prompt|model|StrOutputParser()

retriever = db.as_retriever()

prompt = ChatPromptTemplate.from_template('''\
以下の文脈だけを踏まえて質問に回答してください。

文脈："""
{context}
"""

質問：{question}
''')

hyde_rag_chain = {
    "question": RunnablePassthrough(),
    "context": hypothetical_chain|retriever,
}|prompt|model|StrOutputParser()

hyde_rag_chain.invoke("LangChainの概要を教えて")

from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

class QueryGenerationOutput(BaseModel):
  queries: list[str] = Field(..., description="検索クエリのリスト")

query_generation_prompt = ChatPromptTemplate.from_template("""\
質問に対してベクターデータベースから関連文書を検索するために、
３つの異なる検索クエリを生成してください。
距離ベースの類似性検索の限界を克服するために、
ユーザの質問に対して複数の視点を提供することが目標です。

質問：{question}
""")

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

query_generation_chain = (
  query_generation_prompt
  |model.with_structured_output(QueryGenerationOutput)
  |(lambda x: x.queries)
)

retriever = db.as_retriever()

prompt = ChatPromptTemplate.from_template('''\
以下の文脈だけを踏まえて質問に回答してください。

文脈："""
{context}
"""

質問：{question}
''')

multi_query_rag_chain = {
    "question": RunnablePassthrough(),
    "context": query_generation_chain|retriever.map(),
}|prompt|model|StrOutputParser()

multi_query_rag_chain.invoke("LangChainの概要を教えて")

from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document

def reciprocal_rank_fusion(
    retriever_outputs: list[list[Document]],
    k: int = 60,
) -> list[str]:
  # 各ドキュメントのコンテンツ（文字列）とそのスコアの対応を保持する辞書を準備
  content_score_mapping = {}

  for docs in retriever_outputs:
    for rank, doc in enumerate(docs):
      content = doc.page_content

      if content not in content_score_mapping:
        content_score_mapping[content] = 0

      content_score_mapping[content] += 1 / (rank + k)

  ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)
  return [content for content, _ in ranked]

class QueryGenerationOutput(BaseModel):
  queries: list[str] = Field(..., description="検索クエリのリスト")

query_generation_prompt = ChatPromptTemplate.from_template("""\
質問に対してベクターデータベースから関連文書を検索するために、
３つの異なる検索クエリを生成してください。
距離ベースの類似性検索の限界を克服するために、
ユーザの質問に対して複数の視点を提供することが目標です。

質問：{question}
""")

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

query_generation_chain = (
  query_generation_prompt
  |model.with_structured_output(QueryGenerationOutput)
  |(lambda x: x.queries)
)

retriever = db.as_retriever()

prompt = ChatPromptTemplate.from_template('''\
以下の文脈だけを踏まえて質問に回答してください。

文脈："""
{context}
"""

質問：{question}
''')

rag_fusion_chain = {
    "question": RunnablePassthrough(),
    "context": query_generation_chain|retriever.map()|reciprocal_rank_fusion,
}|prompt|model|StrOutputParser()

rag_fusion_chain.invoke("LangChainの概要を教えて")

from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from typing import Any
from langchain_cohere import CohereRerank

def rerank(inp: dict[str, Any], top_n: int = 3) -> list[Document]:
  question = inp["question"]
  documents = inp["documents"]

  cohere_reranker = CohereRerank(model="rerank-multilingual-v3.0", top_n=top_n)
  return cohere_reranker.compress_documents(documents=documents, query=question)

retriever = db.as_retriever()

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

prompt = ChatPromptTemplate.from_template('''\
以下の文脈だけを踏まえて質問に回答してください。

文脈："""
{context}
"""

質問：{question}
''')

rerank_rag_chain = (
    {
        "question": RunnablePassthrough(),
        "documents": retriever,
    }
    |RunnablePassthrough.assign(context=rerank)
    |prompt|model|StrOutputParser()
)

rerank_rag_chain.invoke("LangChainの概要を教えて")

from pydantic import BaseModel
from enum import Enum
from typing import Any
from langchain_community.retrievers import TavilySearchAPIRetriever
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document

retriever = db.as_retriever()
langchain_document_retriever = retriever.with_config(
    {"run_name": "langchain_document_retriever"}
)

web_retriever =  TavilySearchAPIRetriever(k=3).with_config(
    {"run_name": "web_retriever"}
)

class Route(str, Enum):
  langchain_document = "langchain_document"
  web = "web"

class RouteOutput(BaseModel):
  route: Route

route_prompt = ChatPromptTemplate.from_template("""\
質問に回答するために適切なRetrieverを選択してください。

質問：{question}
""")

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

route_chain = (
    route_prompt
    |model.with_structured_output(RouteOutput)
    |(lambda x: x.route)
)

def routed_retriever(inp: dict[str, Any]) -> list[Document]:
  question = inp["question"]
  route = inp["route"]

  if route == Route.langchain_document:
    return langchain_document_retriever.invoke(question)
  elif route == Route.web:
    return web_retriever.invoke(question)

  raise ValueError(f"Unknown retriever: {retriever}")

prompt = ChatPromptTemplate.from_template('''\
以下の文脈だけを踏まえて質問に回答してください。

文脈："""
{context}
"""

質問：{question}
''')

route_rag_chain = (
    {
        "question": RunnablePassthrough(),
        "route": route_chain,
    }
    |RunnablePassthrough.assign(context=routed_retriever)
    |prompt|model|StrOutputParser()
)

route_rag_chain.invoke("LangChainの概要を教えて")

from langchain_community.retrievers import BM25Retriever
from langchain_core.runnables import RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

retriever = db.as_retriever()
chroma_retriever = retriever.with_config(
    {"run_name": "chroma_retriever"}
)

bm25_retriever = BM25Retriever.from_documents(documents).with_config(
    {"run_name": "bm25_retriever"}
)

hybrid_retriever = (
    RunnableParallel({
        "chroma_documents": chroma_retriever,
        "bm25_documents": bm25_retriever,
    })
    |(lambda x: [x["chroma_documents"], x["bm25_documents"]])
    |reciprocal_rank_fusion
)

prompt = ChatPromptTemplate.from_template('''\
以下の文脈だけを踏まえて質問に回答してください。

文脈："""
{context}
"""

質問：{question}
''')

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

hybrid_rag_chain = (
    {
        "question": RunnablePassthrough(),
        "context": hybrid_retriever,
    }
    |prompt|model|StrOutputParser()
)

hybrid_rag_chain.invoke("LangChainの概要を教えて")